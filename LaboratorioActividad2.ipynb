{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdQSmVdnIo7zb+7XNASnVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darwinyusef/UsaHousingLab/blob/master/LaboratorioActividad2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow --upgrade\n",
        "!pip install keras\n",
        "!pip install sklearn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "rayPsBl1hMY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "XElCYX-zisWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua_bTqhlcGoP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Input, Dense\n",
        "# from keras.optimizers import Adam\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://darwinyusef.github.io/UsaHousingLab/creditcardcsvpresent.csv"
      ],
      "metadata": {
        "id": "tX7k0vR9p9uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducci√≥n\n",
        "\n",
        "En este laboratorio se realiz√≥ un an√°lisis de segmentaci√≥n de clientes basado en un dataset relacionado con transacciones de tarjetas de cr√©dito. El objetivo principal fue aplicar t√©cnicas de clustering (agrupamiento no supervisado) con el algoritmo K-Means para identificar patrones o grupos de clientes que presenten comportamientos similares.\n",
        "\n",
        "Este tipo de an√°lisis es √∫til en sistemas de detecci√≥n de fraude, segmentaci√≥n de marketing o an√°lisis de riesgo crediticio."
      ],
      "metadata": {
        "id": "yPxmkyjJsETr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuente del Dataset\n",
        "# Dataset: Credit Card Fraud Detection\n",
        "\n",
        "Abstract Data Set for Credit Card Fraud Detection\n",
        "\n",
        "Link del CSV utilizado:\n",
        "https://github.com/darwinyusef/UsaHousingLab/blob/master/creditcardcsvpresent.csv"
      ],
      "metadata": {
        "id": "uTDzlyiUsK0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now read the CSV file\n",
        "df = pd.read_csv(\"/content/creditcardcsvpresent.csv\", sep=',')\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "E-cChq6Gc1vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() # actualmente tiene 3075 elementos"
      ],
      "metadata": {
        "id": "sz13Xf4WenH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripci√≥n Detallada del Dataset\n",
        "\n",
        "El dataset utilizado corresponde a un problema cl√°sico de detecci√≥n de fraude en transacciones con tarjetas de cr√©dito. Contiene un total de 31 columnas y fue preprocesado para proteger la confidencialidad de los clientes mediante t√©cnicas de anonimizaci√≥n.\n",
        "\n",
        "### Caracter√≠sticas Generales:\n",
        "\n",
        "- Total de filas (instancias de transacciones): 284,807\n",
        "- Total de columnas (atributos): 31\n",
        "- La mayor√≠a de las variables han sido transformadas usando An√°lisis de Componentes Principales (PCA) para preservar la privacidad de los datos reales.\n",
        "\n",
        "---\n",
        "\n",
        "### Descripci√≥n de las Variables\n",
        "\n",
        "| Variable | Descripci√≥n | Tipo de Dato |\n",
        "|----------|-------------|--------------|\n",
        "| Time     | Tiempo en segundos transcurrido desde la primera transacci√≥n registrada en el dataset. | Num√©rico |\n",
        "| V1 a V28 | Componentes principales generados por PCA sobre las variables originales. No se conoce el significado exacto por razones de confidencialidad. | Num√©rico |\n",
        "| Amount   | Monto de la transacci√≥n realizada. | Num√©rico |\n",
        "| Class    | Variable objetivo que indica el tipo de transacci√≥n: 0 = Transacci√≥n leg√≠tima, 1 = Transacci√≥n fraudulenta. | Binario (0 o 1) |\n",
        "\n",
        "---\n",
        "\n",
        "### Caracter√≠sticas Importantes del Dataset:\n",
        "\n",
        "- Es un dataset desbalanceado:  \n",
        "  - 492 transacciones fraudulentas (0.17%)  \n",
        "  - 284,315 transacciones leg√≠timas (99.83%)  \n",
        "\n",
        "- Variables V1 a V28 permiten captar patrones ocultos de comportamiento transaccional gracias a la t√©cnica de PCA.\n",
        "\n",
        "- La variable `Amount` puede requerir normalizaci√≥n o estandarizaci√≥n previa a la aplicaci√≥n de algoritmos de clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ed3BS6tVs5of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estad√≠sticas de las variables num√©ricas\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "H8-4--pFdzTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Identificar columnas categ√≥ricas.\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "print(cat_cols)\n",
        "# Frecuencia de categor√≠as\n",
        "for col in cat_cols:\n",
        "    print(f\"Columna: {col}\")\n",
        "    print(df[col].value_counts())\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "UHubvgTad7Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5eP03HfQw99n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An√°lisis Exploratorio: Matriz de Correlaci√≥n\n",
        "\n",
        "Antes de aplicar cualquier t√©cnica de detecci√≥n de outliers o clustering, se realiz√≥ un an√°lisis de correlaci√≥n entre las variables m√°s relevantes del dataset. Esto permite entender la relaci√≥n entre las caracter√≠sticas y detectar redundancias o dependencias.\n",
        "\n",
        "### Observaciones relevantes:\n",
        "\n",
        "- Las variables **`Daily_chargeback_avg_amt`**, **`6_month_avg_chbk_amt`** y **`6-month_chbk_freq`** presentan una **alta correlaci√≥n positiva** entre s√≠ (mayor a 0.85). Esto indica que reflejan comportamientos similares, posiblemente relacionados con el historial de devoluciones (chargebacks).\n",
        "- Variables como **`Total Number of declines/day`** o **`Merchant_id`** muestran **baja correlaci√≥n** con el resto, indicando independencia, lo que podr√≠a ser √∫til para detectar anomal√≠as espec√≠ficas de ciertos comercios o h√°bitos poco frecuentes Yo elimine una de ellas.\n",
        "\n",
        "**Seleccionar variables representativas** y evitar redundancia  **Reducir imensiones** de forma m√°s efectiva al aplicar **PCA**. **Interpretar los clusters** y validar que variables como los montos promedio o las devoluciones tienen peso real en la agrupaci√≥n.\n"
      ],
      "metadata": {
        "id": "RzSi9k6p2veB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de correlaci√≥n\n",
        "corr = df.corr(numeric_only=True)\n",
        "\n",
        "# Mapa de calor\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Matriz de Correlaci√≥n\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FHOD0WjjeA-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la figura 2 se muestra un mapa de calor y en la figura 3 una matriz de correlaciones. Analiza en detalle estos datos. ¬øQu√© variables convertir√≠a a categ√≥ricas o factor? ¬øQu√© variables eliminar√≠a? Justifica la respuesta.\n",
        "\n",
        "![Matris de ](https://raw.githubusercontent.com/darwinyusef/UsaHousingLab/refs/heads/master/corrconfianza.JPG)\n",
        "\n",
        "La matriz de correlaciones muestra c√≥mo se relacionan entre s√≠ las variables num√©ricas del dataset. Los valores cercanos a 1 o -1 indican una fuerte correlaci√≥n, mientras que los cercanos a 0 indican poca o ninguna relaci√≥n lineal.\n",
        "\n",
        "Merchant_id no tiene correlaci√≥n significativa con otras variables.\n",
        "\n",
        "Transaction_amount est√° moderadamente correlacionada con Average.Amount.transaction.day.\n",
        "\n",
        "Las variables relacionadas con \"chargeback\" (Daily_chargeback_avg_amt, X6_month_avg_chbk_amt, X6.month_chbk_freq) est√°n fuertemente correlacionadas entre s√≠ (cerca de 0.9).\n",
        "\n",
        "Total.Number.of.declines.day no muestra una correlaci√≥n fuerte con otras variables.\n",
        "\n",
        "## 2. Variables que se pueden convertir a categ√≥ricas (factores)\n",
        "\n",
        "| Variable                     | ¬øConvertir a categ√≥rica? | Justificaci√≥n                                                                 |\n",
        "|-----------------------------|---------------------------|-------------------------------------------------------------------------------|\n",
        "| `Merchant_id`               | ‚úÖ S√≠                     | Es un identificador, no tiene sentido como variable num√©rica.                |\n",
        "| `Total.Number.of.declines.day` | ‚ö†Ô∏è Tal vez              | Si los valores son enteros peque√±os, podr√≠a tratarse como ordinal.          |\n",
        "| `X6.month_chbk_freq`        | ‚ö†Ô∏è Tal vez                | Si tiene pocos valores √∫nicos, puede considerarse como categ√≥rica.           |\n",
        "\n",
        "\n",
        "## 3. Variables que se pueden eliminar\n",
        "\n",
        "| Variable                     | ¬øEliminar? | Justificaci√≥n                                                                 |\n",
        "|-----------------------------|------------|-------------------------------------------------------------------------------|\n",
        "| `Merchant_id`               | ‚úÖ S√≠      | No tiene valor anal√≠tico en an√°lisis num√©rico; solo es √∫til como identificador. |\n",
        "| `Daily_chargeback_avg_amt` | ‚ö†Ô∏è Potencial | Alta correlaci√≥n con `X6_month_avg_chbk_amt`; puede causar redundancia.      |\n",
        "| `X6_month_avg_chbk_amt`     | ‚ö†Ô∏è Potencial | Mismo motivo que arriba; mantener solo una de las dos.                       |\n",
        "| `Transaction_amount` o `Average.Amount.transaction.day` | ‚ùå No | Aunque correlacionadas, aportan informaci√≥n complementaria.                  |\n",
        "\n",
        "Yo literal mate a Transaction_amount y Merchant_id"
      ],
      "metadata": {
        "id": "Ub7tmKUJ7JTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecci√≥n de Anomal√≠as\n",
        "\n",
        "\n",
        "La detecci√≥n de anomal√≠as, tambi√©n conocida como detecci√≥n de outliers, es una t√©cnica fundamental en el an√°lisis de datos. Permite identificar comportamientos at√≠picos o sospechosos que se desv√≠an significativamente del patr√≥n general. En el contexto de los datos financieros, como las transacciones con tarjetas de cr√©dito, detectar estas anomal√≠as puede ayudar a prevenir fraudes o errores operacionales.\n",
        "\n",
        "Para este estudio, se aplicaron diversas t√©cnicas de clustering y detecci√≥n de anomal√≠as, tanto con reducci√≥n de dimensionalidad (PCA) como sin ella, y se evalu√≥ su rendimiento e interpretabilidad.\n",
        "\n",
        "### üß≠ Enfoque del An√°lisis\n",
        "\n",
        "1. **Preprocesamiento de los datos**:\n",
        "   - Estandarizaci√≥n de variables.\n",
        "   - An√°lisis exploratorio para identificar posibles valores extremos."
      ],
      "metadata": {
        "id": "yUQG8JFm3Dc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisi√≥n de valores nulos\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "v2tDDKr_ednR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Esto debido a que tiene muchisimos nulls es el 100% y se recomienda borrarla cuando tiene (m√°s del 40% del total).\n",
        "df.drop(columns=['Transaction date', 'Merchant_id'], inplace=True)\n",
        "\n",
        "# Verificamos que se elimin√≥\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h26nxJxxe0WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellenar valores nulos con la media (opcionalmente la mediana)\n",
        "for col in df.select_dtypes(include=[np.number]).columns:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "for col in cat_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "rHUJRR2nfH3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2. **Aplicaci√≥n de t√©cnicas de detecci√≥n de outliers**:\n",
        "   - **Isolation Forest** para detectar puntos que se a√≠slan f√°cilmente.\n",
        "   - **Local Outlier Factor (LOF)** para encontrar observaciones con baja densidad local.\n",
        "   - **Autoencoder** para identificar errores altos de reconstrucci√≥n en los datos.\n"
      ],
      "metadata": {
        "id": "sDC-Myo1z2vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ## Isolation Forest\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "**Descripci√≥n**: Algoritmo basado en √°rboles que a√≠sla las observaciones an√≥malas. Funciona construyendo √°rboles de aislamiento aleatorios; los puntos que requieren menos divisiones para aislarse son considerados anomal√≠as.\n",
        "\n",
        "finalmente es Eficiente en datasets grandes y de alta dimensi√≥n. No requiere etiquetas. es muy comun para Detecci√≥n de fraudes, errores en sensores, valores at√≠picos en series temporales."
      ],
      "metadata": {
        "id": "0o30YjAeyGWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Selecci√≥n de variables num√©ricas\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Modelo Isolation Forest\n",
        "iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "df['anomaly'] = iso.fit_predict(df[num_cols])\n",
        "\n",
        "# Interpretaci√≥n: -1 = An√≥malo, 1 = Normal\n",
        "print(df['anomaly'].value_counts())\n",
        "\n",
        "# Visualizaci√≥n de anomal√≠as\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='anomaly', data=df)\n",
        "plt.title('Anomal√≠as detectadas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yo8OE8hYfg-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_scores = stats.zscore(df[num_cols])\n",
        "abs_z_scores = np.abs(z_scores)\n",
        "outliers = (abs_z_scores > 3).any(axis=1)\n",
        "\n",
        "df['anomaly_z'] = np.where(outliers, -1, 1)\n",
        "df['anomaly_z']\n"
      ],
      "metadata": {
        "id": "jaY8333Eg9NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T1RJod83yRpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normaliza los datos\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Definici√≥n del Autoencoder\n",
        "input_dim = df_scaled.shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(8, activation=\"relu\")(input_layer)\n",
        "decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "\n",
        "# Compilar\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Entrenar solo con datos normales (sin outliers)\n",
        "autoencoder.fit(df_scaled, df_scaled, epochs=50, batch_size=32, verbose=1)\n"
      ],
      "metadata": {
        "id": "mpTPjD7ehVpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ## Local Outlier Factor (LOF)\n",
        "\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "**Descripci√≥n**: Eval√∫a la anomal√≠a de cada muestra comparando su densidad local con la de sus vecinos. Una muestra se considera an√≥mala si tiene una densidad significativamente menor que la de sus vecinos.\n",
        "\n",
        "Captura relaciones locales; √∫til cuando las anomal√≠as no son globales pero s√≠ en ciertas regiones del espacio de caracter√≠sticas."
      ],
      "metadata": {
        "id": "lhXVctaYyWE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
        "df['anomaly_lof'] = lof.fit_predict(df[num_cols])\n",
        "print(df['anomaly_lof'])\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='anomaly_lof', data=df)\n",
        "plt.title('Anomal√≠as LOF')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "55z30l3jg0_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ## Autoencoder  (Red Neuronal no supervisada)\n",
        "\n",
        "**Descripci√≥n**: Red neuronal no supervisada que aprende a comprimir (codificar) y reconstruir (decodificar) los datos. Las muestras que no pueden ser reconstruidas correctamente se consideran an√≥malas.\n",
        "\n",
        "Ventajas: Muy potente para datos de alta dimensi√≥n. Puede capturar patrones complejos no lineales.\n",
        "\n",
        "M√©trica clave: Se mide el error de reconstrucci√≥n; si este error es mayor a un umbral, la instancia se considera una anomal√≠a."
      ],
      "metadata": {
        "id": "7SMcyJcGylue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "input_dim = df[num_cols].shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(8, activation=\"relu\")(input_layer)\n",
        "decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "reconstructions = autoencoder.predict(df_scaled)\n",
        "\n",
        "# Error cuadr√°tico medio por fila\n",
        "mse = np.mean(np.power(df_scaled - reconstructions, 2), axis=1)\n",
        "\n",
        "# Establecer un umbral para considerar anomal√≠as\n",
        "threshold = np.percentile(mse, 99)  # Top 1% como outliers\n",
        "\n",
        "df['anomaly_autoencoder'] = np.where(mse > threshold, -1, 1)\n",
        "\n",
        "print(df['anomaly_autoencoder'].value_counts())\n",
        "\n",
        "\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='anomaly_autoencoder', data=df)\n",
        "plt.title('Anomal√≠as detectadas con Autoencoder')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hJYL8JLOg5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "T√©cnicas de Clustering Utilizadas\n",
        "\n",
        "| **T√©cnica**                  | **Entrada**          | **Visualizaci√≥n** | **Observaciones clave**                        |\n",
        "| ---------------------------- | -------------------- | ----------------- | ---------------------------------------------- |\n",
        "| **DBSCAN Sin PCA**           | Variables originales | 2D                | Detecta formas arbitrarias y outliers.         |\n",
        "| **KMeans Con PCA**           | Datos reducidos      | PCA 2D            | R√°pido, pero pierde interpretabilidad directa. |\n",
        "| **Agglomerative Clustering** | Variables originales | Dendrograma       | Muestra jerarqu√≠a de agrupaci√≥n.               |\n",
        "| **KMeans Sin PCA**           | Variables originales | 2D                | M√°s interpretable, pero sensible a outliers.   |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fm8O0lzO0vkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## ---"
      ],
      "metadata": {
        "id": "f484N5RKyvUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An√°lisis de Clusters con KMeans y T√©cnicas de Detecci√≥n de Outliers\n",
        "\n",
        "En este an√°lisis se utiliz√≥ **KMeans**, uno de los algoritmos de clustering m√°s populares, para segmentar los datos de transacciones de tarjetas de cr√©dito. Sin embargo, KMeans por s√≠ solo es **sensible a la presencia de outliers**, ya que estos pueden alterar la posici√≥n de los centroides y afectar negativamente la calidad del agrupamiento.\n",
        "\n",
        "Para mitigar este problema, se aplicaron t√©cnicas de **detecci√≥n de anomal√≠as previas al clustering**, logrando una mejor separaci√≥n entre grupos y una identificaci√≥n m√°s precisa de comportamientos an√≥malos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. **Eliminaci√≥n de outliers detectados** o **etiquetado para an√°lisis paralelo**.\n",
        "\n",
        "4. **Clustering con KMeans**:\n",
        "   - Sin reducci√≥n de dimensiones: se mantuvo la interpretaci√≥n directa de las variables.\n",
        "   - Con PCA: se aplic√≥ reducci√≥n de dimensiones para acelerar el entrenamiento y facilitar la visualizaci√≥n.\n",
        "\n",
        "5. **Visualizaci√≥n de los resultados**:\n",
        "   - Gr√°ficas 2D de los clusters formados.\n",
        "   - Comparaci√≥n de agrupamientos con y sin PCA.\n",
        "   - Validaci√≥n del n√∫mero √≥ptimo de clusters con el **Elbow Method**.\n",
        "\n",
        "---\n",
        "\n",
        "### Finalmente explico\n",
        "\n",
        "- La aplicaci√≥n de t√©cnicas de outlier **mejor√≥ considerablemente** la coherencia de los clusters generados por KMeans.\n",
        "- El uso de **PCA** facilit√≥ la visualizaci√≥n en 2D, aunque a costa de perder interpretabilidad directa de los ejes.\n",
        "- KMeans fue capaz de segmentar de forma eficiente a los usuarios, especialmente tras eliminar las anomal√≠as m√°s evidentes.\n",
        "\n",
        "---\n",
        "\n",
        "Este enfoque h√≠brido entre clustering y detecci√≥n de anomal√≠as proporciona un pipeline robusto para detectar fraudes o transacciones at√≠picas en sistemas financieros.\n"
      ],
      "metadata": {
        "id": "eDjZoaLVyup-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecci√≥n de variables num√©ricas\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Escalar los datos (KMeans es sensible a las magnitudes)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Elegimos 3 Clusters como ejemplo\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "# Revisar la cantidad de registros por cluster\n",
        "print(df['cluster'].value_counts())\n",
        "\n",
        "# Visualizaci√≥n de los Clusters\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', data=df)\n",
        "plt.title('Clustering con KMeans')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvAevtlqi-LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[['Transaction_amount', 'Average Amount/transaction/day']])"
      ],
      "metadata": {
        "id": "beie2_w7kI9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis comparativa con el uso de ELBOW METHOD metrics\n",
        "\n",
        "\n",
        "Para determinar el n√∫mero adecuado de clusters al aplicar KMeans, se utiliz√≥ el m√©todo del codo (Elbow Method), que eval√∫a la inercia (suma de las distancias cuadradas internas dentro de los clusters) para distintos valores de k.\n",
        "\n",
        "Observaciones del Gr√°fico\n",
        "En el gr√°fico se observa una disminuci√≥n pronunciada de la inercia hasta k = 3, momento en el que la curva empieza a aplanarse.\n",
        "\n",
        "Este \"codo\" en la curva sugiere que k = 3 es una buena elecci√≥n, ya que incrementar el n√∫mero de clusters m√°s all√° de ese punto no reduce significativamente la inercia."
      ],
      "metadata": {
        "id": "dd8xEZAMxz4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "distortions = []\n",
        "K = range(1, 10)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(df_scaled)\n",
        "    distortions.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('N√∫mero de Clusters k')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Elbow Method para determinar k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N8_hugHnkLQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ahora podemos ver lo siguiente:\n",
        "\n",
        "## Cluster\tComportamiento Detectado\n",
        "0.\tClientes con bajo consumo y transacciones bajas.\n",
        "1.\tClientes con consumo medio.\n",
        "2.\tClientes con alto consumo. Son los VIP o an√≥malos."
      ],
      "metadata": {
        "id": "GJCJZhpIlOV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', data=df)\n",
        "plt.title('Nuevo Clustering con KMeans Sin PCA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEqseXWqkQIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis aplicaci√≥n de cluster sobre las caracter√≠sticas originales, sin reducci√≥n de dimensionalidad (PCA).\n",
        "\n",
        "\n",
        "Transaction_amount (Monto total de transacciones)\n",
        "\n",
        "Average Amount/transaction/day (Promedio diario por transacci√≥n)\n",
        "\n",
        "Descripci√≥n:\n",
        "\n",
        "Se aplic√≥ KMeans directamente sobre las caracter√≠sticas originales, sin reducci√≥n de dimensionalidad (PCA).\n",
        "\n",
        "+ El modelo agrup√≥ los datos en 3 clusters:\n",
        "\n",
        "+ Cluster 0 (negro): transacciones de bajo monto y baja frecuencia diaria.\n",
        "\n",
        "+ Cluster 1 (rojo oscuro): montos y frecuencias intermedias.\n",
        "\n",
        "+ Cluster 2 (rosado claro): incluye las transacciones m√°s altas y tambi√©n las m√°s espor√°dicas.\n",
        "\n",
        "An√°lisis:\n",
        "\n",
        "El cluster m√°s claro (rosado claro) contiene valores extremos, lo cual podr√≠a indicar anomal√≠as o transacciones at√≠picas (potencialmente fraudulentas).\n",
        "El clustering sin PCA ofrece una visi√≥n m√°s directa del comportamiento financiero basado en las m√©tricas reales.\n",
        "Aunque KMeans no est√° dise√±ado para detectar outliers, los puntos dispersos en el cluster 2 sugieren una mayor varianza.\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "Resultados m√°s interpretables en funci√≥n de los valores reales.\n",
        "√ötil para analizar directamente el impacto del monto y frecuencia sin transformar los datos.\n",
        "\n",
        "Limitaciones:\n",
        "\n",
        "KMeans es sensible a la escala de los datos y a outliers.\n",
        "Supone que los clusters son esf√©ricos y de tama√±o similar, lo cual puede no reflejar la estructura real de los datos."
      ],
      "metadata": {
        "id": "UhGwg5CKv7z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pruebas realizadas con StandardScaler y KMeans con metrics silhouette_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Cargar datos\n",
        "df2 = pd.read_csv('/content/creditcardcsvpresent.csv')\n",
        "\n",
        "# An√°lisis exploratorio\n",
        "print(df2.info())\n",
        "print(df2.describe())\n",
        "\n",
        "# Selecci√≥n de variables num√©ricas\n",
        "num_cols = ['Transaction_amount', 'Average Amount/transaction/day']\n",
        "X = df2[num_cols]\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df2['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualizaci√≥n\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df2, x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', palette='rocket')\n",
        "plt.title('Clustering con KMeans Sin PCA')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XQ7KDbEOl2S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "df2['cluster'] = clusters\n",
        "\n",
        "\n",
        "sns.scatterplot(data=df, x=\"Transaction_amount\", y=\"Average Amount/transaction/day\", hue=\"cluster\", palette=\"tab10\")\n",
        "plt.title(\"Clustering con DBSCAN Sin PCA\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o3lejzN5mhSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Dataset de ejemplo (supongo que ya tienes tu DataFrame llamado df)\n",
        "num_cols = ['Transaction_amount', 'Average Amount/transaction/day']\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Crear el modelo\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "clusters = agg_clustering.fit_predict(X_scaled)\n",
        "\n",
        "# Agregar los clusters al DataFrame\n",
        "df['cluster'] = clusters\n"
      ],
      "metadata": {
        "id": "ZTTw3aLZnHOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Clustering Jer√°rquico - Agglomerative Clustering Sin PCA\n",
        "Transaction_amount (Monto total de transacciones)\n",
        "Average Amount/transaction/day (Promedio diario por transacci√≥n)\n",
        "## Observaciones:\n",
        "\n",
        "DBSCAN identific√≥ 3 clusters principales.\n",
        "\n",
        "Es capaz de detectar grupos con forma arbitraria y distinguir outliers (aunque no se visualizan expl√≠citamente en negro, podr√≠an estar filtrados).\n",
        "\n",
        "El grupo verde (Cluster 2) incluye valores extremos, posiblemente relacionados con transacciones at√≠picas o fraudulentas.\n",
        "\n",
        "Los otros dos grupos (azul y naranja) representan clientes con patrones de gasto m√°s regulares.\n",
        "\n",
        "Ventaja: no necesita definir el n√∫mero de clusters y es robusto frente a outliers.\n",
        "\n",
        "Limitaci√≥n: sensible a la elecci√≥n de eps y min_samples."
      ],
      "metadata": {
        "id": "z17lpmIEu46Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df, x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', palette='tab10')\n",
        "plt.title(\"Clustering Jer√°rquico - Agglomerative Clustering Sin PCA\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5epD5IIynJwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dendrograma jer√°rquico que representa la distancia entre las muestras.\n",
        "\n",
        "Observaciones:\n",
        "Muestra c√≥mo se forman los clusters mediante fusiones sucesivas.\n",
        "\n",
        "En este dendrograma, un corte a cierta altura (por ejemplo, en el eje Y cerca de 60) sugiere una divisi√≥n en dos clusters grandes.\n",
        "Ideal para analizar la estructura de los datos, aunque poco escalable para grandes vol√∫menes.\n",
        "Ventaja: no requiere definir el n√∫mero de clusters de entrada.\n",
        "\n",
        "\n",
        "Se realiza de manera representativa la definici√≥n de outliers"
      ],
      "metadata": {
        "id": "srQcbMbXviU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linked = linkage(X_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(linked,\n",
        "           orientation='top',\n",
        "           distance_sort='descending',\n",
        "           show_leaf_counts=False)\n",
        "plt.title('Dendrograma - Agglomerative Clustering')\n",
        "plt.xlabel('Muestras')\n",
        "plt.ylabel('Distancia')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HwgfEF0-nMoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caracter√≠sticas usadas:\n",
        "Datos reducidos mediante PCA (Componentes Principales) ‚Üí proyecci√≥n en 2D.\n",
        "+ KMeans Clustering\n",
        "+ DBSCAN Clustering\n",
        "+ Agglomerative Clustering\n",
        "Observaciones:\n",
        "\n",
        "Se agruparon los datos en 3 clusters.\n",
        "\n",
        "La proyecci√≥n PCA muestra dos grupos claramente densos (rojo y azul) y un grupo m√°s disperso (verde).\n",
        "\n",
        "El grupo verde (Cluster 2) probablemente representa comportamientos de gasto m√°s variables o inusuales.\n",
        "\n",
        "Este m√©todo es eficiente computacionalmente pero sensible a la escala y a outliers.\n",
        "\n",
        "Ventaja: r√°pido y f√°cil de implementar.\n",
        "\n",
        "Limitaci√≥n: supone que los clusters son esf√©ricos y de tama√±o similar."
      ],
      "metadata": {
        "id": "y3o8tdmkvO1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cargar el dataset\n",
        "df = pd.read_csv('/content/creditcardcsvpresent.csv')  # Cambia el path si es necesario\n",
        "\n",
        "# Eliminamos columnas con muchos nulos\n",
        "df = df.drop(columns=['Transaction date'])\n",
        "\n",
        "# Selecci√≥n de variables num√©ricas\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Escalado de datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Reducci√≥n de dimensionalidad a 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Clustering con KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['kmeans_cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Clustering con DBSCAN\n",
        "dbscan = DBSCAN(eps=2, min_samples=5)\n",
        "df['dbscan_cluster'] = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Clustering con Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "df['agg_cluster'] = agg.fit_predict(X_scaled)\n",
        "\n",
        "# Funci√≥n para graficar clusters\n",
        "def plot_clusters(X, labels, title):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels, palette='Set1', legend='full')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Gr√°ficas\n",
        "plot_clusters(X_pca, df['kmeans_cluster'], 'KMeans Clustering')\n",
        "plot_clusters(X_pca, df['dbscan_cluster'], 'DBSCAN Clustering')\n",
        "plot_clusters(X_pca, df['agg_cluster'], 'Agglomerative Clustering')\n"
      ],
      "metadata": {
        "id": "EYSw8sI7nor6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans Clustering - Reducci√≥n de dimensionalidad con PCA\n",
        "\n",
        "## An√°lisis de Componentes Principales\n",
        "\n",
        "Visualizaci√≥n: Proyecci√≥n bidimensional (2D) de los datos transformados por PCA.\n",
        "\n",
        "üìä Observaciones del Gr√°fico\n",
        "Se identifican tres cl√∫steres distintos, visualmente separados:\n",
        "\n",
        "Cluster 0 (rojo): Agrupa una gran parte de las observaciones centradas alrededor del origen con forma de tri√°ngulo invertido.\n",
        "\n",
        "Cluster 1 (azul): A la izquierda del rojo, con una orientaci√≥n algo m√°s vertical.\n",
        "\n",
        "Cluster 2 (verde): Abarca un espacio m√°s amplio, incluyendo puntos dispersos, lo que sugiere mayor variabilidad o presencia de outliers.\n",
        "\n",
        "üí° Ventajas del uso de PCA\n",
        "Reducci√≥n de ruido y dimensiones permite visualizar de manera clara las separaciones de los grupos.\n",
        "\n",
        "Facilita la interpretaci√≥n visual del agrupamiento.\n",
        "\n",
        "‚ö†Ô∏è Limitaciones\n",
        "P√©rdida de interpretabilidad directa: Las nuevas dimensiones (componentes principales) no representan atributos originales como transaction_amount o avg_amount_per_day, dificultando la explicaci√≥n sem√°ntica del agrupamiento.\n",
        "\n",
        "Outliers visibles: El cluster 2 incluye puntos alejados del centro, lo cual puede afectar la precisi√≥n del modelo si no se gestionan adecuadamente."
      ],
      "metadata": {
        "id": "uq94fUZoxAy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Cargar tus datos\n",
        "df = pd.read_csv('/content/creditcardcsvpresent.csv')\n",
        "\n",
        "# 2. Seleccionar las variables num√©ricas que te interesan\n",
        "X = df[['Transaction_amount', 'Average Amount/transaction/day']]  # Change this line\n",
        "\n",
        "# 3. Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 4. Aplicar PCA\n",
        "pca = PCA(n_components=2)  # Reducimos a 2 dimensiones para graficar\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 5. Aplicar Clustering\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "clusters_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "clusters_agg = agg.fit_predict(X_pca)\n",
        "\n",
        "# 6. Graficar KMeans\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=clusters_kmeans, palette='Set1')\n",
        "plt.title('Clustering con KMeans (PCA)')\n",
        "plt.show()\n",
        "\n",
        "# 7. Graficar Agglomerative\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=clusters_agg, palette='Set2')\n",
        "plt.title('Clustering Jer√°rquico - Agglomerative (PCA)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "50uoMUx9o6gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Objetivo de Uso del Dataset en este Laboratorio\n",
        "\n",
        "Aplicar t√©cnicas de aprendizaje no supervisado (K-Means Clustering) para explorar posibles agrupaciones de las transacciones y analizar si existen patrones de comportamiento diferenciados entre transacciones leg√≠timas y fraudulentas.\n",
        "\n",
        "Esto permitir√° evaluar la utilidad del clustering como t√©cnica exploratoria dentro de un sistema de an√°lisis de fraudes.\n",
        "\n",
        "---\n",
        "\n",
        "Mediante este trabajo se pretende que pongas en pr√°ctica la aplicaci√≥n de los algoritmos de detecci√≥n de anomal√≠as u outliers y las t√©cnicas de agrupamiento. El objetivo es que comprendas de forma pr√°ctica con un problema determinado los pasos que hay que realizar para detecci√≥n autom√°tica de valores inusuales y, por otro lado, analizar los cl√∫ster o grupos resultado de aplicar un algoritmo de agrupamiento."
      ],
      "metadata": {
        "id": "12ObBlGOtAnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **T√©cnica**                  | **Entrada**          | **Visualizaci√≥n** | **Observaciones clave**                        |\n",
        "| ---------------------------- | -------------------- | ----------------- | ---------------------------------------------- |\n",
        "| **DBSCAN Sin PCA**           | Variables originales | 2D                | Detecta formas arbitrarias y outliers.         |\n",
        "| **KMeans Con PCA**           | Datos reducidos      | PCA 2D            | R√°pido, pero pierde interpretabilidad directa. |\n",
        "| **Agglomerative Clustering** | Variables originales | Dendrograma       | Muestra jerarqu√≠a de agrupaci√≥n.               |\n",
        "| **KMeans Sin PCA**           | Variables originales | 2D                | M√°s interpretable, pero sensible a outliers.   |"
      ],
      "metadata": {
        "id": "tV_9aL5OwtUy"
      }
    }
  ]
}