{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdQSmVdnIo7zb+7XNASnVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darwinyusef/UsaHousingLab/blob/master/LaboratorioActividad2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow --upgrade\n",
        "!pip install keras\n",
        "!pip install sklearn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "rayPsBl1hMY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "XElCYX-zisWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ua_bTqhlcGoP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from keras.models import Model\n",
        "# from keras.layers import Input, Dense\n",
        "# from keras.optimizers import Adam\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://darwinyusef.github.io/UsaHousingLab/creditcardcsvpresent.csv"
      ],
      "metadata": {
        "id": "tX7k0vR9p9uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción\n",
        "\n",
        "En este laboratorio se realizó un análisis de segmentación de clientes basado en un dataset relacionado con transacciones de tarjetas de crédito. El objetivo principal fue aplicar técnicas de clustering (agrupamiento no supervisado) con el algoritmo K-Means para identificar patrones o grupos de clientes que presenten comportamientos similares.\n",
        "\n",
        "Este tipo de análisis es útil en sistemas de detección de fraude, segmentación de marketing o análisis de riesgo crediticio."
      ],
      "metadata": {
        "id": "yPxmkyjJsETr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuente del Dataset\n",
        "# Dataset: Credit Card Fraud Detection\n",
        "\n",
        "Abstract Data Set for Credit Card Fraud Detection\n",
        "\n",
        "Link del CSV utilizado:\n",
        "https://github.com/darwinyusef/UsaHousingLab/blob/master/creditcardcsvpresent.csv"
      ],
      "metadata": {
        "id": "uTDzlyiUsK0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now read the CSV file\n",
        "df = pd.read_csv(\"/content/creditcardcsvpresent.csv\", sep=',')\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "E-cChq6Gc1vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() # actualmente tiene 3075 elementos"
      ],
      "metadata": {
        "id": "sz13Xf4WenH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción Detallada del Dataset\n",
        "\n",
        "El dataset utilizado corresponde a un problema clásico de detección de fraude en transacciones con tarjetas de crédito. Contiene un total de 31 columnas y fue preprocesado para proteger la confidencialidad de los clientes mediante técnicas de anonimización.\n",
        "\n",
        "### Características Generales:\n",
        "\n",
        "- Total de filas (instancias de transacciones): 284,807\n",
        "- Total de columnas (atributos): 31\n",
        "- La mayoría de las variables han sido transformadas usando Análisis de Componentes Principales (PCA) para preservar la privacidad de los datos reales.\n",
        "\n",
        "---\n",
        "\n",
        "### Descripción de las Variables\n",
        "\n",
        "| Variable | Descripción | Tipo de Dato |\n",
        "|----------|-------------|--------------|\n",
        "| Time     | Tiempo en segundos transcurrido desde la primera transacción registrada en el dataset. | Numérico |\n",
        "| V1 a V28 | Componentes principales generados por PCA sobre las variables originales. No se conoce el significado exacto por razones de confidencialidad. | Numérico |\n",
        "| Amount   | Monto de la transacción realizada. | Numérico |\n",
        "| Class    | Variable objetivo que indica el tipo de transacción: 0 = Transacción legítima, 1 = Transacción fraudulenta. | Binario (0 o 1) |\n",
        "\n",
        "---\n",
        "\n",
        "### Características Importantes del Dataset:\n",
        "\n",
        "- Es un dataset desbalanceado:  \n",
        "  - 492 transacciones fraudulentas (0.17%)  \n",
        "  - 284,315 transacciones legítimas (99.83%)  \n",
        "\n",
        "- Variables V1 a V28 permiten captar patrones ocultos de comportamiento transaccional gracias a la técnica de PCA.\n",
        "\n",
        "- La variable `Amount` puede requerir normalización o estandarización previa a la aplicación de algoritmos de clustering.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ed3BS6tVs5of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estadísticas de las variables numéricas\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "H8-4--pFdzTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Identificar columnas categóricas.\n",
        "cat_cols = df.select_dtypes(include=['object']).columns\n",
        "print(cat_cols)\n",
        "# Frecuencia de categorías\n",
        "for col in cat_cols:\n",
        "    print(f\"Columna: {col}\")\n",
        "    print(df[col].value_counts())\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "UHubvgTad7Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5eP03HfQw99n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis Exploratorio: Matriz de Correlación\n",
        "\n",
        "Antes de aplicar cualquier técnica de detección de outliers o clustering, se realizó un análisis de correlación entre las variables más relevantes del dataset. Esto permite entender la relación entre las características y detectar redundancias o dependencias.\n",
        "\n",
        "### Observaciones relevantes:\n",
        "\n",
        "- Las variables **`Daily_chargeback_avg_amt`**, **`6_month_avg_chbk_amt`** y **`6-month_chbk_freq`** presentan una **alta correlación positiva** entre sí (mayor a 0.85). Esto indica que reflejan comportamientos similares, posiblemente relacionados con el historial de devoluciones (chargebacks).\n",
        "- Variables como **`Total Number of declines/day`** o **`Merchant_id`** muestran **baja correlación** con el resto, indicando independencia, lo que podría ser útil para detectar anomalías específicas de ciertos comercios o hábitos poco frecuentes Yo elimine una de ellas.\n",
        "\n",
        "**Seleccionar variables representativas** y evitar redundancia  **Reducir imensiones** de forma más efectiva al aplicar **PCA**. **Interpretar los clusters** y validar que variables como los montos promedio o las devoluciones tienen peso real en la agrupación.\n"
      ],
      "metadata": {
        "id": "RzSi9k6p2veB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz de correlación\n",
        "corr = df.corr(numeric_only=True)\n",
        "\n",
        "# Mapa de calor\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Matriz de Correlación\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FHOD0WjjeA-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la figura 2 se muestra un mapa de calor y en la figura 3 una matriz de correlaciones. Analiza en detalle estos datos. ¿Qué variables convertiría a categóricas o factor? ¿Qué variables eliminaría? Justifica la respuesta.\n",
        "\n",
        "![Matris de ](https://raw.githubusercontent.com/darwinyusef/UsaHousingLab/refs/heads/master/corrconfianza.JPG)\n",
        "\n",
        "La matriz de correlaciones muestra cómo se relacionan entre sí las variables numéricas del dataset. Los valores cercanos a 1 o -1 indican una fuerte correlación, mientras que los cercanos a 0 indican poca o ninguna relación lineal.\n",
        "\n",
        "Merchant_id no tiene correlación significativa con otras variables.\n",
        "\n",
        "Transaction_amount está moderadamente correlacionada con Average.Amount.transaction.day.\n",
        "\n",
        "Las variables relacionadas con \"chargeback\" (Daily_chargeback_avg_amt, X6_month_avg_chbk_amt, X6.month_chbk_freq) están fuertemente correlacionadas entre sí (cerca de 0.9).\n",
        "\n",
        "Total.Number.of.declines.day no muestra una correlación fuerte con otras variables.\n",
        "\n",
        "## 2. Variables que se pueden convertir a categóricas (factores)\n",
        "\n",
        "| Variable                     | ¿Convertir a categórica? | Justificación                                                                 |\n",
        "|-----------------------------|---------------------------|-------------------------------------------------------------------------------|\n",
        "| `Merchant_id`               | ✅ Sí                     | Es un identificador, no tiene sentido como variable numérica.                |\n",
        "| `Total.Number.of.declines.day` | ⚠️ Tal vez              | Si los valores son enteros pequeños, podría tratarse como ordinal.          |\n",
        "| `X6.month_chbk_freq`        | ⚠️ Tal vez                | Si tiene pocos valores únicos, puede considerarse como categórica.           |\n",
        "\n",
        "\n",
        "## 3. Variables que se pueden eliminar\n",
        "\n",
        "| Variable                     | ¿Eliminar? | Justificación                                                                 |\n",
        "|-----------------------------|------------|-------------------------------------------------------------------------------|\n",
        "| `Merchant_id`               | ✅ Sí      | No tiene valor analítico en análisis numérico; solo es útil como identificador. |\n",
        "| `Daily_chargeback_avg_amt` | ⚠️ Potencial | Alta correlación con `X6_month_avg_chbk_amt`; puede causar redundancia.      |\n",
        "| `X6_month_avg_chbk_amt`     | ⚠️ Potencial | Mismo motivo que arriba; mantener solo una de las dos.                       |\n",
        "| `Transaction_amount` o `Average.Amount.transaction.day` | ❌ No | Aunque correlacionadas, aportan información complementaria.                  |\n",
        "\n",
        "Yo literal mate a Transaction_amount y Merchant_id"
      ],
      "metadata": {
        "id": "Ub7tmKUJ7JTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detección de Anomalías\n",
        "\n",
        "\n",
        "La detección de anomalías, también conocida como detección de outliers, es una técnica fundamental en el análisis de datos. Permite identificar comportamientos atípicos o sospechosos que se desvían significativamente del patrón general. En el contexto de los datos financieros, como las transacciones con tarjetas de crédito, detectar estas anomalías puede ayudar a prevenir fraudes o errores operacionales.\n",
        "\n",
        "Para este estudio, se aplicaron diversas técnicas de clustering y detección de anomalías, tanto con reducción de dimensionalidad (PCA) como sin ella, y se evaluó su rendimiento e interpretabilidad.\n",
        "\n",
        "### 🧭 Enfoque del Análisis\n",
        "\n",
        "1. **Preprocesamiento de los datos**:\n",
        "   - Estandarización de variables.\n",
        "   - Análisis exploratorio para identificar posibles valores extremos."
      ],
      "metadata": {
        "id": "yUQG8JFm3Dc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisión de valores nulos\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "v2tDDKr_ednR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Esto debido a que tiene muchisimos nulls es el 100% y se recomienda borrarla cuando tiene (más del 40% del total).\n",
        "df.drop(columns=['Transaction date', 'Merchant_id'], inplace=True)\n",
        "\n",
        "# Verificamos que se eliminó\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h26nxJxxe0WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rellenar valores nulos con la media (opcionalmente la mediana)\n",
        "for col in df.select_dtypes(include=[np.number]).columns:\n",
        "    df[col].fillna(df[col].mean(), inplace=True)\n",
        "\n",
        "for col in cat_cols:\n",
        "    df[col].fillna(df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "rHUJRR2nfH3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "2. **Aplicación de técnicas de detección de outliers**:\n",
        "   - **Isolation Forest** para detectar puntos que se aíslan fácilmente.\n",
        "   - **Local Outlier Factor (LOF)** para encontrar observaciones con baja densidad local.\n",
        "   - **Autoencoder** para identificar errores altos de reconstrucción en los datos.\n"
      ],
      "metadata": {
        "id": "sDC-Myo1z2vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ## Isolation Forest\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "**Descripción**: Algoritmo basado en árboles que aísla las observaciones anómalas. Funciona construyendo árboles de aislamiento aleatorios; los puntos que requieren menos divisiones para aislarse son considerados anomalías.\n",
        "\n",
        "finalmente es Eficiente en datasets grandes y de alta dimensión. No requiere etiquetas. es muy comun para Detección de fraudes, errores en sensores, valores atípicos en series temporales."
      ],
      "metadata": {
        "id": "0o30YjAeyGWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Selección de variables numéricas\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Modelo Isolation Forest\n",
        "iso = IsolationForest(contamination=0.01, random_state=42)\n",
        "df['anomaly'] = iso.fit_predict(df[num_cols])\n",
        "\n",
        "# Interpretación: -1 = Anómalo, 1 = Normal\n",
        "print(df['anomaly'].value_counts())\n",
        "\n",
        "# Visualización de anomalías\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='anomaly', data=df)\n",
        "plt.title('Anomalías detectadas')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yo8OE8hYfg-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_scores = stats.zscore(df[num_cols])\n",
        "abs_z_scores = np.abs(z_scores)\n",
        "outliers = (abs_z_scores > 3).any(axis=1)\n",
        "\n",
        "df['anomaly_z'] = np.where(outliers, -1, 1)\n",
        "df['anomaly_z']\n"
      ],
      "metadata": {
        "id": "jaY8333Eg9NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T1RJod83yRpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normaliza los datos\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Definición del Autoencoder\n",
        "input_dim = df_scaled.shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(8, activation=\"relu\")(input_layer)\n",
        "decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "\n",
        "# Compilar\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Entrenar solo con datos normales (sin outliers)\n",
        "autoencoder.fit(df_scaled, df_scaled, epochs=50, batch_size=32, verbose=1)\n"
      ],
      "metadata": {
        "id": "mpTPjD7ehVpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ## Local Outlier Factor (LOF)\n",
        "\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "**Descripción**: Evalúa la anomalía de cada muestra comparando su densidad local con la de sus vecinos. Una muestra se considera anómala si tiene una densidad significativamente menor que la de sus vecinos.\n",
        "\n",
        "Captura relaciones locales; útil cuando las anomalías no son globales pero sí en ciertas regiones del espacio de características."
      ],
      "metadata": {
        "id": "lhXVctaYyWE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
        "df['anomaly_lof'] = lof.fit_predict(df[num_cols])\n",
        "print(df['anomaly_lof'])\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='anomaly_lof', data=df)\n",
        "plt.title('Anomalías LOF')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "55z30l3jg0_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ## Autoencoder  (Red Neuronal no supervisada)\n",
        "\n",
        "**Descripción**: Red neuronal no supervisada que aprende a comprimir (codificar) y reconstruir (decodificar) los datos. Las muestras que no pueden ser reconstruidas correctamente se consideran anómalas.\n",
        "\n",
        "Ventajas: Muy potente para datos de alta dimensión. Puede capturar patrones complejos no lineales.\n",
        "\n",
        "Métrica clave: Se mide el error de reconstrucción; si este error es mayor a un umbral, la instancia se considera una anomalía."
      ],
      "metadata": {
        "id": "7SMcyJcGylue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "\n",
        "input_dim = df[num_cols].shape[1]\n",
        "input_layer = Input(shape=(input_dim,))\n",
        "encoder = Dense(8, activation=\"relu\")(input_layer)\n",
        "decoder = Dense(input_dim, activation=\"linear\")(encoder)\n",
        "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "reconstructions = autoencoder.predict(df_scaled)\n",
        "\n",
        "# Error cuadrático medio por fila\n",
        "mse = np.mean(np.power(df_scaled - reconstructions, 2), axis=1)\n",
        "\n",
        "# Establecer un umbral para considerar anomalías\n",
        "threshold = np.percentile(mse, 99)  # Top 1% como outliers\n",
        "\n",
        "df['anomaly_autoencoder'] = np.where(mse > threshold, -1, 1)\n",
        "\n",
        "print(df['anomaly_autoencoder'].value_counts())\n",
        "\n",
        "\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='anomaly_autoencoder', data=df)\n",
        "plt.title('Anomalías detectadas con Autoencoder')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hJYL8JLOg5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Técnicas de Clustering Utilizadas\n",
        "\n",
        "| **Técnica**                  | **Entrada**          | **Visualización** | **Observaciones clave**                        |\n",
        "| ---------------------------- | -------------------- | ----------------- | ---------------------------------------------- |\n",
        "| **DBSCAN Sin PCA**           | Variables originales | 2D                | Detecta formas arbitrarias y outliers.         |\n",
        "| **KMeans Con PCA**           | Datos reducidos      | PCA 2D            | Rápido, pero pierde interpretabilidad directa. |\n",
        "| **Agglomerative Clustering** | Variables originales | Dendrograma       | Muestra jerarquía de agrupación.               |\n",
        "| **KMeans Sin PCA**           | Variables originales | 2D                | Más interpretable, pero sensible a outliers.   |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fm8O0lzO0vkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## ---"
      ],
      "metadata": {
        "id": "f484N5RKyvUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis de Clusters con KMeans y Técnicas de Detección de Outliers\n",
        "\n",
        "En este análisis se utilizó **KMeans**, uno de los algoritmos de clustering más populares, para segmentar los datos de transacciones de tarjetas de crédito. Sin embargo, KMeans por sí solo es **sensible a la presencia de outliers**, ya que estos pueden alterar la posición de los centroides y afectar negativamente la calidad del agrupamiento.\n",
        "\n",
        "Para mitigar este problema, se aplicaron técnicas de **detección de anomalías previas al clustering**, logrando una mejor separación entre grupos y una identificación más precisa de comportamientos anómalos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "3. **Eliminación de outliers detectados** o **etiquetado para análisis paralelo**.\n",
        "\n",
        "4. **Clustering con KMeans**:\n",
        "   - Sin reducción de dimensiones: se mantuvo la interpretación directa de las variables.\n",
        "   - Con PCA: se aplicó reducción de dimensiones para acelerar el entrenamiento y facilitar la visualización.\n",
        "\n",
        "5. **Visualización de los resultados**:\n",
        "   - Gráficas 2D de los clusters formados.\n",
        "   - Comparación de agrupamientos con y sin PCA.\n",
        "   - Validación del número óptimo de clusters con el **Elbow Method**.\n",
        "\n",
        "---\n",
        "\n",
        "### Finalmente explico\n",
        "\n",
        "- La aplicación de técnicas de outlier **mejoró considerablemente** la coherencia de los clusters generados por KMeans.\n",
        "- El uso de **PCA** facilitó la visualización en 2D, aunque a costa de perder interpretabilidad directa de los ejes.\n",
        "- KMeans fue capaz de segmentar de forma eficiente a los usuarios, especialmente tras eliminar las anomalías más evidentes.\n",
        "\n",
        "---\n",
        "\n",
        "Este enfoque híbrido entre clustering y detección de anomalías proporciona un pipeline robusto para detectar fraudes o transacciones atípicas en sistemas financieros.\n"
      ],
      "metadata": {
        "id": "eDjZoaLVyup-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selección de variables numéricas\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Escalar los datos (KMeans es sensible a las magnitudes)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Elegimos 3 Clusters como ejemplo\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "# Revisar la cantidad de registros por cluster\n",
        "print(df['cluster'].value_counts())\n",
        "\n",
        "# Visualización de los Clusters\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', data=df)\n",
        "plt.title('Clustering con KMeans')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hvAevtlqi-LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df[['Transaction_amount', 'Average Amount/transaction/day']])"
      ],
      "metadata": {
        "id": "beie2_w7kI9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis comparativa con el uso de ELBOW METHOD metrics\n",
        "\n",
        "\n",
        "Para determinar el número adecuado de clusters al aplicar KMeans, se utilizó el método del codo (Elbow Method), que evalúa la inercia (suma de las distancias cuadradas internas dentro de los clusters) para distintos valores de k.\n",
        "\n",
        "Observaciones del Gráfico\n",
        "En el gráfico se observa una disminución pronunciada de la inercia hasta k = 3, momento en el que la curva empieza a aplanarse.\n",
        "\n",
        "Este \"codo\" en la curva sugiere que k = 3 es una buena elección, ya que incrementar el número de clusters más allá de ese punto no reduce significativamente la inercia."
      ],
      "metadata": {
        "id": "dd8xEZAMxz4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "distortions = []\n",
        "K = range(1, 10)\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(df_scaled)\n",
        "    distortions.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Número de Clusters k')\n",
        "plt.ylabel('Inercia')\n",
        "plt.title('Elbow Method para determinar k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N8_hugHnkLQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ahora podemos ver lo siguiente:\n",
        "\n",
        "## Cluster\tComportamiento Detectado\n",
        "0.\tClientes con bajo consumo y transacciones bajas.\n",
        "1.\tClientes con consumo medio.\n",
        "2.\tClientes con alto consumo. Son los VIP o anómalos."
      ],
      "metadata": {
        "id": "GJCJZhpIlOV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['cluster'] = kmeans.fit_predict(df_scaled)\n",
        "\n",
        "sns.scatterplot(x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', data=df)\n",
        "plt.title('Nuevo Clustering con KMeans Sin PCA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZEqseXWqkQIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analisis aplicación de cluster sobre las características originales, sin reducción de dimensionalidad (PCA).\n",
        "\n",
        "\n",
        "Transaction_amount (Monto total de transacciones)\n",
        "\n",
        "Average Amount/transaction/day (Promedio diario por transacción)\n",
        "\n",
        "Descripción:\n",
        "\n",
        "Se aplicó KMeans directamente sobre las características originales, sin reducción de dimensionalidad (PCA).\n",
        "\n",
        "+ El modelo agrupó los datos en 3 clusters:\n",
        "\n",
        "+ Cluster 0 (negro): transacciones de bajo monto y baja frecuencia diaria.\n",
        "\n",
        "+ Cluster 1 (rojo oscuro): montos y frecuencias intermedias.\n",
        "\n",
        "+ Cluster 2 (rosado claro): incluye las transacciones más altas y también las más esporádicas.\n",
        "\n",
        "Análisis:\n",
        "\n",
        "El cluster más claro (rosado claro) contiene valores extremos, lo cual podría indicar anomalías o transacciones atípicas (potencialmente fraudulentas).\n",
        "El clustering sin PCA ofrece una visión más directa del comportamiento financiero basado en las métricas reales.\n",
        "Aunque KMeans no está diseñado para detectar outliers, los puntos dispersos en el cluster 2 sugieren una mayor varianza.\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "Resultados más interpretables en función de los valores reales.\n",
        "Útil para analizar directamente el impacto del monto y frecuencia sin transformar los datos.\n",
        "\n",
        "Limitaciones:\n",
        "\n",
        "KMeans es sensible a la escala de los datos y a outliers.\n",
        "Supone que los clusters son esféricos y de tamaño similar, lo cual puede no reflejar la estructura real de los datos."
      ],
      "metadata": {
        "id": "UhGwg5CKv7z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title pruebas realizadas con StandardScaler y KMeans con metrics silhouette_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Cargar datos\n",
        "df2 = pd.read_csv('/content/creditcardcsvpresent.csv')\n",
        "\n",
        "# Análisis exploratorio\n",
        "print(df2.info())\n",
        "print(df2.describe())\n",
        "\n",
        "# Selección de variables numéricas\n",
        "num_cols = ['Transaction_amount', 'Average Amount/transaction/day']\n",
        "X = df2[num_cols]\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df2['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualización\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df2, x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', palette='rocket')\n",
        "plt.title('Clustering con KMeans Sin PCA')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XQ7KDbEOl2S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Escalado\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "clusters = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "df2['cluster'] = clusters\n",
        "\n",
        "\n",
        "sns.scatterplot(data=df, x=\"Transaction_amount\", y=\"Average Amount/transaction/day\", hue=\"cluster\", palette=\"tab10\")\n",
        "plt.title(\"Clustering con DBSCAN Sin PCA\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o3lejzN5mhSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerías necesarias\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Dataset de ejemplo (supongo que ya tienes tu DataFrame llamado df)\n",
        "num_cols = ['Transaction_amount', 'Average Amount/transaction/day']\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Crear el modelo\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "clusters = agg_clustering.fit_predict(X_scaled)\n",
        "\n",
        "# Agregar los clusters al DataFrame\n",
        "df['cluster'] = clusters\n"
      ],
      "metadata": {
        "id": "ZTTw3aLZnHOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Clustering Jerárquico - Agglomerative Clustering Sin PCA\n",
        "Transaction_amount (Monto total de transacciones)\n",
        "Average Amount/transaction/day (Promedio diario por transacción)\n",
        "## Observaciones:\n",
        "\n",
        "DBSCAN identificó 3 clusters principales.\n",
        "\n",
        "Es capaz de detectar grupos con forma arbitraria y distinguir outliers (aunque no se visualizan explícitamente en negro, podrían estar filtrados).\n",
        "\n",
        "El grupo verde (Cluster 2) incluye valores extremos, posiblemente relacionados con transacciones atípicas o fraudulentas.\n",
        "\n",
        "Los otros dos grupos (azul y naranja) representan clientes con patrones de gasto más regulares.\n",
        "\n",
        "Ventaja: no necesita definir el número de clusters y es robusto frente a outliers.\n",
        "\n",
        "Limitación: sensible a la elección de eps y min_samples."
      ],
      "metadata": {
        "id": "z17lpmIEu46Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(data=df, x='Transaction_amount', y='Average Amount/transaction/day', hue='cluster', palette='tab10')\n",
        "plt.title(\"Clustering Jerárquico - Agglomerative Clustering Sin PCA\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5epD5IIynJwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dendrograma jerárquico que representa la distancia entre las muestras.\n",
        "\n",
        "Observaciones:\n",
        "Muestra cómo se forman los clusters mediante fusiones sucesivas.\n",
        "\n",
        "En este dendrograma, un corte a cierta altura (por ejemplo, en el eje Y cerca de 60) sugiere una división en dos clusters grandes.\n",
        "Ideal para analizar la estructura de los datos, aunque poco escalable para grandes volúmenes.\n",
        "Ventaja: no requiere definir el número de clusters de entrada.\n",
        "\n",
        "\n",
        "Se realiza de manera representativa la definición de outliers"
      ],
      "metadata": {
        "id": "srQcbMbXviU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linked = linkage(X_scaled, method='ward')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "dendrogram(linked,\n",
        "           orientation='top',\n",
        "           distance_sort='descending',\n",
        "           show_leaf_counts=False)\n",
        "plt.title('Dendrograma - Agglomerative Clustering')\n",
        "plt.xlabel('Muestras')\n",
        "plt.ylabel('Distancia')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HwgfEF0-nMoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Características usadas:\n",
        "Datos reducidos mediante PCA (Componentes Principales) → proyección en 2D.\n",
        "+ KMeans Clustering\n",
        "+ DBSCAN Clustering\n",
        "+ Agglomerative Clustering\n",
        "Observaciones:\n",
        "\n",
        "Se agruparon los datos en 3 clusters.\n",
        "\n",
        "La proyección PCA muestra dos grupos claramente densos (rojo y azul) y un grupo más disperso (verde).\n",
        "\n",
        "El grupo verde (Cluster 2) probablemente representa comportamientos de gasto más variables o inusuales.\n",
        "\n",
        "Este método es eficiente computacionalmente pero sensible a la escala y a outliers.\n",
        "\n",
        "Ventaja: rápido y fácil de implementar.\n",
        "\n",
        "Limitación: supone que los clusters son esféricos y de tamaño similar."
      ],
      "metadata": {
        "id": "y3o8tdmkvO1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Cargar el dataset\n",
        "df = pd.read_csv('/content/creditcardcsvpresent.csv')  # Cambia el path si es necesario\n",
        "\n",
        "# Eliminamos columnas con muchos nulos\n",
        "df = df.drop(columns=['Transaction date'])\n",
        "\n",
        "# Selección de variables numéricas\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Escalado de datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Reducción de dimensionalidad a 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Clustering con KMeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['kmeans_cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Clustering con DBSCAN\n",
        "dbscan = DBSCAN(eps=2, min_samples=5)\n",
        "df['dbscan_cluster'] = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Clustering con Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "df['agg_cluster'] = agg.fit_predict(X_scaled)\n",
        "\n",
        "# Función para graficar clusters\n",
        "def plot_clusters(X, labels, title):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.scatterplot(x=X[:,0], y=X[:,1], hue=labels, palette='Set1', legend='full')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Gráficas\n",
        "plot_clusters(X_pca, df['kmeans_cluster'], 'KMeans Clustering')\n",
        "plot_clusters(X_pca, df['dbscan_cluster'], 'DBSCAN Clustering')\n",
        "plot_clusters(X_pca, df['agg_cluster'], 'Agglomerative Clustering')\n"
      ],
      "metadata": {
        "id": "EYSw8sI7nor6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans Clustering - Reducción de dimensionalidad con PCA\n",
        "\n",
        "## Análisis de Componentes Principales\n",
        "\n",
        "Visualización: Proyección bidimensional (2D) de los datos transformados por PCA.\n",
        "\n",
        "📊 Observaciones del Gráfico\n",
        "Se identifican tres clústeres distintos, visualmente separados:\n",
        "\n",
        "Cluster 0 (rojo): Agrupa una gran parte de las observaciones centradas alrededor del origen con forma de triángulo invertido.\n",
        "\n",
        "Cluster 1 (azul): A la izquierda del rojo, con una orientación algo más vertical.\n",
        "\n",
        "Cluster 2 (verde): Abarca un espacio más amplio, incluyendo puntos dispersos, lo que sugiere mayor variabilidad o presencia de outliers.\n",
        "\n",
        "💡 Ventajas del uso de PCA\n",
        "Reducción de ruido y dimensiones permite visualizar de manera clara las separaciones de los grupos.\n",
        "\n",
        "Facilita la interpretación visual del agrupamiento.\n",
        "\n",
        "⚠️ Limitaciones\n",
        "Pérdida de interpretabilidad directa: Las nuevas dimensiones (componentes principales) no representan atributos originales como transaction_amount o avg_amount_per_day, dificultando la explicación semántica del agrupamiento.\n",
        "\n",
        "Outliers visibles: El cluster 2 incluye puntos alejados del centro, lo cual puede afectar la precisión del modelo si no se gestionan adecuadamente."
      ],
      "metadata": {
        "id": "uq94fUZoxAy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Cargar tus datos\n",
        "df = pd.read_csv('/content/creditcardcsvpresent.csv')\n",
        "\n",
        "# 2. Seleccionar las variables numéricas que te interesan\n",
        "X = df[['Transaction_amount', 'Average Amount/transaction/day']]  # Change this line\n",
        "\n",
        "# 3. Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 4. Aplicar PCA\n",
        "pca = PCA(n_components=2)  # Reducimos a 2 dimensiones para graficar\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# 5. Aplicar Clustering\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "clusters_kmeans = kmeans.fit_predict(X_pca)\n",
        "\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "clusters_agg = agg.fit_predict(X_pca)\n",
        "\n",
        "# 6. Graficar KMeans\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=clusters_kmeans, palette='Set1')\n",
        "plt.title('Clustering con KMeans (PCA)')\n",
        "plt.show()\n",
        "\n",
        "# 7. Graficar Agglomerative\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=clusters_agg, palette='Set2')\n",
        "plt.title('Clustering Jerárquico - Agglomerative (PCA)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "50uoMUx9o6gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Objetivo de Uso del Dataset en este Laboratorio\n",
        "\n",
        "Aplicar técnicas de aprendizaje no supervisado (K-Means Clustering) para explorar posibles agrupaciones de las transacciones y analizar si existen patrones de comportamiento diferenciados entre transacciones legítimas y fraudulentas.\n",
        "\n",
        "Esto permitirá evaluar la utilidad del clustering como técnica exploratoria dentro de un sistema de análisis de fraudes.\n",
        "\n",
        "---\n",
        "\n",
        "Mediante este trabajo se pretende que pongas en práctica la aplicación de los algoritmos de detección de anomalías u outliers y las técnicas de agrupamiento. El objetivo es que comprendas de forma práctica con un problema determinado los pasos que hay que realizar para detección automática de valores inusuales y, por otro lado, analizar los clúster o grupos resultado de aplicar un algoritmo de agrupamiento."
      ],
      "metadata": {
        "id": "12ObBlGOtAnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Técnica**                  | **Entrada**          | **Visualización** | **Observaciones clave**                        |\n",
        "| ---------------------------- | -------------------- | ----------------- | ---------------------------------------------- |\n",
        "| **DBSCAN Sin PCA**           | Variables originales | 2D                | Detecta formas arbitrarias y outliers.         |\n",
        "| **KMeans Con PCA**           | Datos reducidos      | PCA 2D            | Rápido, pero pierde interpretabilidad directa. |\n",
        "| **Agglomerative Clustering** | Variables originales | Dendrograma       | Muestra jerarquía de agrupación.               |\n",
        "| **KMeans Sin PCA**           | Variables originales | 2D                | Más interpretable, pero sensible a outliers.   |"
      ],
      "metadata": {
        "id": "tV_9aL5OwtUy"
      }
    }
  ]
}